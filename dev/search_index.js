var documenterSearchIndex = {"docs":
[{"location":"#VeryBasicCNN","page":"Home","title":"VeryBasicCNN","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for VeryBasicCNN.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#VeryBasicCNN.HyperParamRanges","page":"Home","title":"VeryBasicCNN.HyperParamRanges","text":"Configuration struct for hyperparameter ranges with current defaults.\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN.HyperParameters","page":"Home","title":"VeryBasicCNN.HyperParameters","text":"HyperParameters\n\nCNN hyperparameters for biological sequence analysis.\n\nFields\n\npfm_len: Length of Position Frequency Matrix (motif length)\nnum_pfms: Number of PFMs (base layer filters)\nnum_img_filters: Number of filters per convolutional layer\nimg_fil_widths: Input channels for each conv layer\nimg_fil_heights: Filter heights for each conv layer\npool_base: Pooling size for base layer\nstride_base: Stride for base layer\npoolsize: Pooling sizes for each conv layer\nstride: Strides for each conv layer\npool_lvl_top: Highest layer that uses pooling\nsoftmax_strength_img_fil: Softmax strength for filter normalization\nbatch_size: Training batch size\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN.LearnedCodeImgFilters","page":"Home","title":"VeryBasicCNN.LearnedCodeImgFilters","text":"LearnedCodeImgFilters\n\nLearnable convolutional filters for intermediate CNN layers.\n\nFields\n\nfilters: 4D array of convolutional filters (height, width, channels, num_filters)\nactivation_scaler: Scalar activation parameter for the layer\n\nConstructor Arguments\n\ninput_channels: Number of input channels (width of previous layer's output)\nfilter_height: Height of convolutional filters\nnum_filters: Number of filters in this layer\ninit_scale: Initialization scale factor\nuse_cuda: Whether to move filters to GPU\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN.LearnedPWMs","page":"Home","title":"VeryBasicCNN.LearnedPWMs","text":"LearnedPWMs\n\nLearnable Position Weight Matrices for the base CNN layer.\n\nFields\n\nfilters: 4D array of PWM filters (height, width, channels, num_filters)\nactivation_thresholds: Activation threshold parameters for each filter\n\nConstructor Arguments\n\nfilter_width: Width of PWM filters (motif length)\nnum_filters: Number of PWM filters to create\ninit_scale: Initialization scale factor (pseudo count)\nuse_cuda: Whether to move filters to GPU\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN.SeqCNN","page":"Home","title":"VeryBasicCNN.SeqCNN","text":"SeqCNN\n\nConvolutional neural network for biological sequence analysis and regression.\n\nThis model implements a multi-layer CNN architecture specifically designed for  biological sequence data (DNA, RNA, proteins) with learnable Position Weight  Matrices (PWMs) as the base layer followed by convolutional layers.\n\nArchitecture Overview\n\nBase Layer: Learnable PWMs for motif detection\nConvolutional Layers: Code image filters with pooling\nFinal Layer: Linear transformation to target outputs\n\nFields\n\nposition_weight_matrices::LearnedPWMs: Base layer PWM filters for motif detection\nconvolutional_filters::Vector{LearnedCodeImgFilters}: Multi-layer conv filters\noutput_weights::AbstractArray{Float,3}: Final linear layer weights (outputdim × embeddim × 1)\noutput_scalers::AbstractArray{Float,1}: Output scaling parameters\n\nConstructor Arguments\n\nhyperparams: HyperParameters containing architecture specification\nsequence_length::Integer: Length of input biological sequences\noutput_dimension=1: Number of output targets (e.g., binding affinities)\ninitialization_scale=0.5: Weight initialization scale factor\nuse_cuda=true: Whether to place model on GPU\n\nExample\n\n# Create model for 41-length DNA sequences with 244 output targets\nhp = generate_random_hyperparameters()\nmodel = SeqCNN(hp, 41; output_dimension=244)\n\n# Forward pass\npredictions = model(sequences)  # Returns (244, batch_size)\n\nNotes\n\nAutomatically calculates embedding dimensions based on architecture\nSupports both single and multi-output regression tasks\nOptimized for biological sequence lengths (typically 20-200 nucleotides)\n\n\n\n\n\n","category":"type"},{"location":"#VeryBasicCNN.SeqCNN-Tuple{Any}","page":"Home","title":"VeryBasicCNN.SeqCNN","text":"(m::SeqCNN)(seq; make_sparse=false, linear_sum=false, predict_position=nothing)\n\nCall overload for SeqCNN models to perform a forward pass and generate predictions.\n\nArguments\n\nseq: Input biological sequences (one-hot encoded or compatible format)\nmake_sparse: If true, applies sparsity-inducing normalization to convolutional filters (default: false)\nlinear_sum: (Unused in this model; included for compatibility)\npredict_position: If provided, returns predictions for a specific output position (default: nothing for all outputs)\n\nReturns\n\nModel predictions as an array of shape (output_dim, batch_size) \nor (batch_size,) for single-output models\n\nNotes\n\nThis enables the model to be called as a function, e.g., preds = model(sequences), for convenient ML-style usage.\n\n\n\n\n\n","category":"method"},{"location":"#Base.getproperty-Tuple{VeryBasicCNN.SeqCNN, Symbol}","page":"Home","title":"Base.getproperty","text":"Base.getproperty(m::SeqCNN, sym::Symbol)\n\nAllow dot-access to batch_size as a virtual field for a SeqCNN model.\n\nmodel.batch_size returns the batch size from the model's hyperparameters (model.hp.batch_size).\nAll other fields are accessed as usual.\n\nThis enables convenient and familiar ML-style access to the batch size without storing redundant fields.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.apply_maxpool_to_code_image-Tuple{Any}","page":"Home","title":"VeryBasicCNN.apply_maxpool_to_code_image","text":"apply_maxpool_to_code_image(code_image; poolsize=(2, 1), stride=(1, 1))\n\nApply max pooling to 4D code image and reshape to maintain tensor structure.\n\nThis function applies max pooling to code images from convolutional layers, automatically calculating the correct output dimensions and reshaping the result to maintain the expected 4D tensor format (height, width, channels, batch).\n\nArguments\n\ncode_image: 4D tensor of code image (height, width, channels, batch)\npoolsize: Pooling kernel size as (height, width) tuple\nstride: Stride of pooling operation as (height, width) tuple\n\nReturns\n\nPooled 4D code image with updated dimensions\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.apply_pooling_to_code-Tuple{Any}","page":"Home","title":"VeryBasicCNN.apply_pooling_to_code","text":"apply_pooling_to_code(code; poolsize=(2,1), stride=(1,1), is_base_layer=false, identity=false, weights=nothing)\n\nApply pooling to CNN code with optional weighting and identity bypass.\n\nArguments\n\ncode: Input code to pool (3D or 4D)\npoolsize: Pooling kernel size, default (2,1) for 1D sequences\nstride: Pooling stride, default (1,1)\nis_base_layer: If true, handles base layer indexing differently\nidentity: If true, bypasses pooling (for skip connections)\nweights: Optional weighting for attention-like mechanisms\n\nReturns\n\nPooled 4D code (height, width, channels, batch)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.calculate_base_layer_output_length-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.calculate_base_layer_output_length","text":"calculate_base_layer_output_length(hyperparams, sequence_length)\n\nCalculate the output length after the base layer (first convolution with PFM filters). Formula: outputlength = sequencelength - pfm_length + 1\n\nThis represents the number of possible positions where a PFM of length pfmlen can be placed on a sequence of length sequencelength.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.calculate_conv_output_length-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.calculate_conv_output_length","text":"calculate_conv_output_length(input_length, filter_length)\n\nCalculate output length after convolution operation. Formula: outputlength = inputlength - filter_length + 1\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.calculate_conv_then_pooled_length-NTuple{4, Any}","page":"Home","title":"VeryBasicCNN.calculate_conv_then_pooled_length","text":"calculate_conv_then_pooled_length(input_length, filter_length, pool_size, stride)\n\nCalculate output length after convolution followed by pooling and stride operations. Combines convolution and pooling calculations: conv → pool\n\nArguments\n\ninput_length: Length of input sequence/tensor\nfilter_length: Length of convolutional filter\npool_size: Size of pooling kernel\nstride: Stride of pooling operation\n\nReturns\n\nFinal output length after convolution and pooling\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.calculate_final_conv_embedding_length-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.calculate_final_conv_embedding_length","text":"calculate_final_conv_embedding_length(hyperparams, sequence_length)\n\nCalculate the embedding length after the final convolutional layer.\n\nThis function simulates the forward pass through all CNN layers to determine the final conv embedding dimension that will be fed to downstream layers (e.g., dense layers):\n\nBase layer: convolution with PFM filters → pooling\nFor each subsequent layer up to poollvltop: conv → pool  \nFor remaining layers: conv only (no pooling)\n\nThe result is the length of the convolutional feature vector produced by the last conv layer, which represents the final conv embedding of the input biological sequence.\n\nArguments\n\nhyperparams: CNN hyperparameters containing layer specifications\nsequence_length: Length of input biological sequence\n\nReturns\n\nFinal conv embedding length (dimension of feature vector from last conv layer)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.calculate_pooled_length-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN.calculate_pooled_length","text":"calculate_pooled_length(input_length, hyperparams, layer_level)\n\nCalculate output length after pooling and striding operations. Formula: outputlength = (inputlength - pool_size) ÷ stride + 1\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.calculate_pooling_output_dimension-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN.calculate_pooling_output_dimension","text":"calculate_pooling_output_dimension(input_dimension, poolsize, stride)\n\nCalculate the output dimension after pooling operation. Standard pooling formula: output = (input - poolsize) ÷ stride + 1\n\nArguments\n\ninput_dimension: Size of input dimension\npoolsize: Size of pooling kernel\nstride: Stride of pooling operation\n\nReturns\n\nOutput dimension after pooling\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.change_batch_size-Tuple{VeryBasicCNN.HyperParameters, Int64}","page":"Home","title":"VeryBasicCNN.change_batch_size","text":"change_batch_size(hp, new_batch_size)\n\nCreate a new HyperParameters instance with updated batch size.\n\nArguments\n\nhp: Original HyperParameters instance\nnew_batch_size: New batch size value\n\nReturns\n\nNew HyperParameters instance with updated batch size\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.codeimg_layer_forward_pass-Tuple{VeryBasicCNN.LearnedCodeImgFilters, Any, VeryBasicCNN.HyperParameters}","page":"Home","title":"VeryBasicCNN.codeimg_layer_forward_pass","text":"codeimg_layer_forward_pass(conv_filters, prev_code_img, hp; return_filters=false, make_sparse=false)\n\nForward pass through a code image convolutional layer.\n\nArguments\n\nconv_filters: LearnedCodeImgFilters instance\nprev_code_img: Previous layer's code image output\nhp: HyperParameters containing layer configuration\nreturn_filters: Whether to return normalized filters along with output\nmake_sparse: Whether to apply sparsity-inducing filter normalization\n\nReturns\n\ncode: Activated code image output\nnormalized_filters: (Optional) Normalized filters if return_filters=true\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.compute_training_loss-Tuple{VeryBasicCNN.SeqCNN, Any, Any}","page":"Home","title":"VeryBasicCNN.compute_training_loss","text":"compute_training_loss(model, hyperparams, sequences, targets; make_sparse=false, verbose=true)\n\nCompute the training objective (Huber loss) for the biological sequence CNN model.\n\nThis function performs a complete forward pass through the model and computes the robust Huber loss for training the CNN on biological sequence data.\n\nArguments\n\nmodel: Trained or training CNN model instance\nsequences: Input biological sequences (typically on GPU)\ntargets: Ground truth target values for regression\nmake_sparse: Whether to apply sparsity-inducing filter normalization\nverbose: Whether to print loss value during training\n\nReturns\n\nScalar loss value for optimization\n\nNotes\n\nUses Huber loss for robustness to outliers in biological data\nHandles NaN values in targets automatically\nSupports sparse filter training for interpretability\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.create_model-Tuple{Any, Any, Int64}","page":"Home","title":"VeryBasicCNN.create_model","text":"create_model(X_dim, Y_dim, batch_size::Int; rng=Random.GLOBAL_RNG, use_cuda::Bool=true)\n\nConstruct a new SeqCNN model with randomly generated hyperparameters and specified data dimensions.\n\nArguments\n\nX_dim::Tuple{Int, Int}: Input data dimensions as (height, sequence_length).\nY_dim::Int: Number of output targets (e.g., regression outputs).\nbatch_size::Int: Batch size for hyperparameter generation.\nrng: (Optional) Random number generator to use for hyperparameter sampling (default: Random.GLOBAL_RNG).\nuse_cuda::Bool: (Optional) Whether to place model parameters on GPU (default: true).\n\nReturns\n\nA new SeqCNN model instance if the final embedding length is valid (≥ 1), otherwise nothing.\n\nNotes\n\nHyperparameters are generated randomly for each call unless a specific RNG is provided.\nReturns nothing if the model architecture is invalid for the given input dimensions.\nThe use_cuda flag controls whether model parameters are moved to GPU.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.create_position_weight_matrix-Tuple{Any}","page":"Home","title":"VeryBasicCNN.create_position_weight_matrix","text":"create_position_weight_matrix(frequencies; background=bg, reverse_comp=false)\n\nCreate a Position Weight Matrix (PWM) from frequency matrix. Converts frequency counts to log2 odds ratios relative to background distribution.\n\nArguments\n\nfrequencies: Matrix of nucleotide/amino acid frequencies\nbackground: Background probability distribution (defaults to uniform)\nreverse_comp: If true, include reverse complement in the PWM\n\nReturns\n\nPosition Weight Matrix with log2 odds ratios\n\nNotes\n\nThis is standard PWM construction: PWM[i,j] = log2(freq[i,j] / background[i])\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.determine_output_weights-Tuple{VeryBasicCNN.SeqCNN}","page":"Home","title":"VeryBasicCNN.determine_output_weights","text":"get the output weights for a given position or all positions\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.final_embedding_length-Union{Tuple{T}, Tuple{Any, Tuple{T, T}}} where T<:Integer","page":"Home","title":"VeryBasicCNN.final_embedding_length","text":"final_embedding_len(hp, X_dim::Tuple{T, T}) where T <: Integer\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.generate_random_hyperparameters-Tuple{}","page":"Home","title":"VeryBasicCNN.generate_random_hyperparameters","text":"generate_random_hyperparameters(; batch_size=nothing, rng=Random.GLOBAL_RNG, ranges=DEFAULT_RANGES)\n\nGenerate random hyperparameters for CNN hyperparameter tuning experiments.\n\nThis function creates randomized but sensible hyperparameter configurations for biological sequence CNN training, useful for hyperparameter search and ablation studies.\n\nArguments\n\nbatch_size: Fixed batch size (if nothing, randomly selected from available options in ranges.batch_size_options)\nrng: Random number generator to use (default: Random.GLOBAL_RNG)\nranges: Struct containing parameter ranges and options (default: DEFAULT_RANGES)\n\nReturns\n\nHyperParameters: Randomized hyperparameter configuration\n\nParameter Ranges (from ranges)\n\npfm_len: motif length range (e.g., 6-15)\nnum_pfms: base layer filter count (e.g., 256-512)\nconv_filter_counts: per-layer filter counts (e.g., 256-512)\nfilter_heights: filter heights for each conv layer (e.g., 2-5)\npooling_sizes: pooling sizes for each conv layer (e.g., 1-2)\nstrides: stride for each conv layer (e.g., 1)\n\nNotes\n\nLast conv layer uses a fixed number of filters for compatibility with final embedding\nPooling parameters are conservative to avoid over-downsampling\nAll stride patterns end with 1 to preserve final resolution\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_input_sequence_length-Tuple{Any}","page":"Home","title":"VeryBasicCNN.get_input_sequence_length","text":"get_input_sequence_length(data)\n\nExtract the input sequence length from data structure.\n\nArguments\n\ndata: Data structure containing sequence matrices\n\nReturns\n\nLength of input sequences (second dimension of first matrix)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_level_dimensions-Tuple{Any}","page":"Home","title":"VeryBasicCNN.get_level_dimensions","text":"get_level_dimensions(tensor; is_base_layer=false)\n\nGet relevant tensor dimensions for CNN layer operations.\n\nFor biological sequence CNNs, this function extracts the dimensions needed for pooling and reshaping operations, handling the different tensor layouts between base layer and subsequent layers.\n\nArguments\n\ntensor: Input tensor (4D: height, width, channels, batch)\nis_base_layer: If true, treats as base layer with different dimension indexing\n\nReturns\n\nTuple of (relevantdim, channels, batchsize) for layer operations\n\nNotes\n\nBase layer: uses dimensions (2, 3, 4) → (width, channels, batch)  \nOther layers: uses dimensions (1, 3, 4) → (height, channels, batch)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_num_conv_layers_above_base-Tuple{Any}","page":"Home","title":"VeryBasicCNN.get_num_conv_layers_above_base","text":"get_num_conv_layers_above_base(hp)\n\nGet the number of convolutional layers above the base layer.\n\nArguments\n\nhp: HyperParameters instance\n\nReturns\n\nNumber of conv layers above base (excludes PFM base layer)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_output_weights-Tuple{VeryBasicCNN.SeqCNN}","page":"Home","title":"VeryBasicCNN.get_output_weights","text":"get_output_weights(model::SeqCNN)\n\nExtract the output layer weights from a SeqCNN model.\n\nArguments\n\nmodel: SeqCNN instance\n\nReturns\n\nOutput weights tensor (outputdim, embeddingdim, 1)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_pooling_size-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.get_pooling_size","text":"get_pooling_size(hyperparams, layer_level)\n\nGet the pooling size for a specific CNN layer.\n\nLayer 0 (base): uses pool_base\nOther layers: uses poolsize[layer_level]\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_pooling_tuple-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.get_pooling_tuple","text":"get_pooling_tuple(hyperparams, layer_level)\n\nGet the pooling size as a tuple (height, width) for convolution operations. Returns (pooling_size, 1) since these are 1D sequences treated as 2D.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_stride_size-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.get_stride_size","text":"get_stride_size(hyperparams, layer_level)\n\nGet the stride size for a specific CNN layer.\n\nLayer 0 (base): uses stride_base  \nOther layers: uses stride[layer_level]\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.get_stride_tuple-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.get_stride_tuple","text":"get_stride_tuple(hyperparams, layer_level)\n\nGet the stride size as a tuple (height, width) for convolution operations. Returns (stride_size, 1) since these are 1D sequences treated as 2D.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.huber_loss-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.huber_loss","text":"huber_loss(predictions, targets; delta=0.85, quadratic_weight=0.5)\n\nCompute Huber loss with NaN handling for robust regression training.\n\nThe Huber loss combines quadratic loss for small errors and linear loss for large errors, making it robust to outliers while remaining smooth and differentiable.\n\nArguments\n\npredictions: Model predictions (ŷ)\ntargets: Ground truth target values (y)  \ndelta: Threshold for switching between quadratic and linear loss\nquadratic_weight: Weight factor for quadratic loss component (default 0.5)\n\nReturns\n\nMean Huber loss over valid (non-NaN) data points\n\nNotes\n\nNaN values in targets are automatically excluded from loss calculation\nFor |error| < delta: loss = quadratic_weight * error²\nFor |error| ≥ delta: loss = delta * (|error| - quadratic_weight * delta)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.initialize_codeimg_activations-Tuple{Any}","page":"Home","title":"VeryBasicCNN.initialize_codeimg_activations","text":"initialize_codeimg_activations(gradient)\n\nInitialize code image activations using ReLU activation.\n\nArguments\n\ngradient: Raw convolution output gradients\n\nReturns\n\nReLU-activated code image\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.masked_mse-Tuple{Any, Any, Any}","page":"Home","title":"VeryBasicCNN.masked_mse","text":"masked_mse(predictions, targets, mask)\n\nCompute mean squared error only on valid (masked) entries.\n\nArguments\n\npredictions: Model predictions\ntargets: Ground truth targets  \nmask: Boolean mask indicating valid entries\n\nReturns\n\nMSE computed only on valid entries specified by mask\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.model_init-Union{Tuple{T}, Tuple{VeryBasicCNN.HyperParameters, Tuple{T, T}, T}} where T<:Integer","page":"Home","title":"VeryBasicCNN.model_init","text":"model_init(hp::HyperParameters, X_dim::Tuple{T, T}, Y_dim::T; kwargs...) where T <: Integer\n\nCreate a SeqCNN model instance given hyperparameters and data dimensions.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.normalize_conv_filters_l2-Tuple{Any}","page":"Home","title":"VeryBasicCNN.normalize_conv_filters_l2","text":"normalize_conv_filters_l2(filters; softmax_strength=SOFTMAX_ALPHA, use_sparsity=false)\n\nNormalize CNN filters using L2 normalization, with optional sparsity-inducing softmax weighting.\n\nArguments\n\nfilters: Convolutional filter weights to normalize\nsoftmax_strength: Strength parameter for softmax sparsity (higher = more sparse)\nuse_sparsity: If true, applies softmax weighting to encourage sparsity\n\nReturns\n\nL2-normalized filters, optionally with sparsity-inducing weights\n\nNotes\n\nL2 normalization helps with gradient stability and convergence\nSparsity option uses softmax to emphasize larger weights and suppress smaller ones\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.normalize_matrix_through_squaring-Tuple{Any}","page":"Home","title":"VeryBasicCNN.normalize_matrix_through_squaring","text":"normalize_matrix_through_squaring(matrix; ϵ=1e-5, reverse_comp=false)\n\nNormalize a matrix by squaring elements and normalizing by column sums.\n\nSquares matrix elements and adds epsilon for numerical stability\nNormalizes by column sums (creates probability-like distribution)\nOptionally creates reverse complement and concatenates along dimension 4\n\nArguments\n\nmatrix: Input matrix to normalize\nϵ: Small value added for numerical stability (prevents division by zero)\nreverse_comp: If true, creates reverse complement and concatenates\n\nReturns\n\nNormalized matrix with optional reverse complement concatenated along dimension 4\n\nNotes\n\nThis creates a probability-like distribution from squared values, useful for  converting frequency matrices to pseudo-probability matrices in PWM construction.\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.predict_from_code-Tuple{Any, Any}","page":"Home","title":"VeryBasicCNN.predict_from_code","text":"predict_from_code(model, code; make_sparse=false, inference_position=nothing)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.predict_from_sequences-Tuple{VeryBasicCNN.SeqCNN, Any}","page":"Home","title":"VeryBasicCNN.predict_from_sequences","text":"predict_from_sequences(hyperparams, model, sequences; make_sparse=false)\n\nComplete forward pass: CNN feature extraction → linear transformation → final predictions.\n\nArguments\n\nhyperparams: CNN hyperparameters\nmodel: Trained CNN model instance  \nsequences: Input biological sequences\nmake_sparse: Apply sparsity-inducing normalization\n\nReturns\n\nModel predictions (outputdim, batchsize)\n\n\n\n\n\n","category":"method"},{"location":"#VeryBasicCNN.prepare_codeimg_layer_params-Tuple{VeryBasicCNN.LearnedCodeImgFilters}","page":"Home","title":"VeryBasicCNN.prepare_codeimg_layer_params","text":"prepare_codeimg_layer_params(conv_filters; alpha=SOFTMAX_ALPHA, make_sparse=false)\n\nPrepare normalized filters and activation scalers for code image convolution layer.\n\nArguments\n\nconv_filters: LearnedCodeImgFilters instance containing raw filters and scalers\nalpha: Softmax strength for sparsity-inducing normalization\nmake_sparse: Whether to apply sparsity-inducing normalization\n\nReturns\n\nnormalized_filters: L2-normalized convolutional filters, optionally with sparsity\nactivation_scaler: Processed activation scaler (squared and clamped)\n\n\n\n\n\n","category":"method"}]
}
